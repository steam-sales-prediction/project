{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Preprocess Metacritic, Genres, and Supported Platforms Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/raw/info_base_games.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Drop Corrupted Sample That Contains The Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[9929]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=9929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Preprocessing the Metacritic Score Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "[Metacritic](https://www.metacritic.com/) is a website for critics and users to review digital content (movies, music, **games**, etc.), the metacritic score that is included on Steam is the **critics** (professional game reviewers) metacritic score, not the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the metacritic column to numeric since it contains NaNs\n",
    "df['metacritic'] = pd.to_numeric(df['metacritic'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Checking Percentage of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df)\n",
    "non_null = df['metacritic'].notna().sum()\n",
    "missing = df['metacritic'].isna().sum()\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"With metacritic score: {non_null} ({non_null/total:.1%})\")\n",
    "print(f\"Missing metacritic score: {missing} ({missing/total:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The above results show that almost 97% of the games in our data don't have a metacritic score associated with them and the metacritic score for them is null, this is due to Steam leaving it **optional** for games' publishers to include a metacritic score on their Steam page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metacritic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['metacritic'].dropna(), bins=20)\n",
    "plt.xlabel(\"Metacritic score\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.title(\"Distribution of Metacritic Scores Ignoring Null Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The above histogram shows that the **metacritic scores are normally distributed**, which means **we can apply standardization on it**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Metacritic scores before standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metacritic'].dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Standardizing metacritic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "# IMPORTANT: later on I should fit_transform only on the training data\n",
    "df['metacritic_preprocessed'] = standardizer.fit_transform(df[['metacritic']])\n",
    "\n",
    "df['metacritic_preprocessed'].dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now a problem remains, which is the missing values, it doesn't make sense to drop all the rows that don't contain a metacritic score since 97% of the data doesn't have it, the best solution that came to my mind is to **set all the NaN metacritic scores** to be equal to the **mean of the standardized metacritic scores**, which is **0**. And create a new boolean feature `has_metacritic` which indicates whether this game has a metacritic score or not, I hope that this can help models understand that if `has_metacritic = 0` then ignore this metacritic score, and also generally `has_metacritic` may later on turn out to be a useful feature on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Creating New `has_metacritic` Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_metacritic'] = df['metacritic'].notna().astype(int)\n",
    "print(df['has_metacritic'].head())\n",
    "print(df['has_metacritic'].loc[[1893]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Replacing Missing Values With the Mean Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metacritic_preprocessed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metacritic_preprocessed'] = df['metacritic_preprocessed'].fillna(0)\n",
    "print(df['metacritic_preprocessed'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Final Thoughts On Preprocessing Metacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The above concludes my current trials to preprocess the metacritic column. I will leave below some thoughts that we may wish to revisit in the future:\n",
    "1. I am not sure if the imputation technique that I used is the most suitable technique for this case, and I am not sure if it is correct to standardize then impute, or should I impute first then standardize, I chose the first approach since I think this will better maintain the distribution of the original data, and especially since I believe the imputed value shouldn't have any more meaning than just indicating that this row didn't have a metacritic value, which I tried to do along with the `has_metacritic` column.\n",
    "\n",
    "2. Most importantly, I believe later on the `metacritic` column **will turn out to not be useful** for our models and that we will remove it as a feature, this is based on some discussions I read ([discussion1](https://Steamcommunity.com/discussions/forum/10/3057367211653181335/?l=latam), [discussion2](https://www.reddit.com/r/pcgaming/comments/1gjadpf/da_tv_metacritic_user_score_vs_Steam_ratings/)) in which multiple people feel that it doesn't give accurate reviews, I also expect that it might have a high correlation with `reviewScore` in `gamalytic_Steam_games.csv`, and considering that a small number of samples have this score, I expect this feature won't be useful, on the other hand, I believe we might find the new `has_metacritic` feature useful on its own, still this is all just speculations and we will find out by using proper feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Preprocessing the Genres Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Split Genres Into Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres_split'] = df['genres'].fillna('')\n",
    "df['genres_split'] = df['genres_split'].apply(lambda x: x.split(', ') if x else [])\n",
    "df['genres_split'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Analyze Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Flatten genres into one big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = [genre\n",
    "              for sublist in df['genres_split']\n",
    "              for genre in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Count genres frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_counts = Counter(all_genres)\n",
    "genre_freq = (\n",
    "    pd.DataFrame.from_dict(genre_counts, orient='index', columns=['count'])\n",
    "      .sort_values('count', ascending=False)\n",
    ")\n",
    "print(\"Number of uniqure genres in the whole dataset:\")\n",
    "print(genre_freq.size)\n",
    "print(genre_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Output word cloud for visualization of the genre counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1400, height=700, background_color='white')\n",
    "wc.generate_from_frequencies(genre_counts)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Multi-Hot Encode The Genres and Merge Them Into the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "# IMPORTANT: later on I should fit_transform only on the training data\n",
    "genres_encoded = mlb.fit_transform(df['genres_split'])\n",
    "genres_df = pd.DataFrame(genres_encoded, columns=[f'genre_{c}' for c in mlb.classes_], index=df.index)\n",
    "df = pd.concat([df, genres_df], axis=1)\n",
    "df = df.drop(columns=['genres_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Final Thoughts On Preprocessing Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "1. There are rare genres (Accounting, Nudity, Web Publishing, etc.), after research I believe we have 2 options that we can do with them, either treat them normally like all the other genres as I did above, or set a certain frequency threshold, and genres that have frequencies less than that threshold get removed and we replace them with a \"genre_Other\" column, I didn't do that since I have a feeling that these rare genres might help during prediction, but the best way to know would be to test both methods in the \"training and evaluation\" phase to determine which method helps the model make better predictions.\n",
    "\n",
    "2. About handling unseen genres that we might get later on with unseen data, if we do not have the \"genre_Other\" column then the best way to handle them would be to ignore them, put 1's in the genres that we know, and ignore the ones we don't know, on the other hand if we have a \"genre_Other\" column, then we would put a 1 at that column.\n",
    "\n",
    "3. If later on the genres prove to be useful in predictions and we select them as a feature, I believe the best way to handle them being missing in unseen data would be to webscrape and get the genres of the game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
